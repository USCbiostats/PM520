{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqotsvX91OXvaABwDkHXtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab_6_ExpFam_Divergences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It's a Family Affair, or: Exponential Families, Natural Gradient Descent, & Statistical Divergences\n",
        "\n",
        "[Exponential Families](https://en.wikipedia.org/wiki/Exponential_family) (sometimes abbreviated as ExpFam) provide a [succinct characterization](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) of many distributions (e.g., [Normal](https://en.wikipedia.org/wiki/Normal_distribution), [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution), [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), [Wishart](https://en.wikipedia.org/wiki/Wishart_distribution), etc.). We'll take an informal look at their properties and how to perform inference.\n",
        "\n",
        "We'll also take an informal review of [statistical divergences](https://en.wikipedia.org/wiki/Divergence_(statistics)), which reflect notions of a \"distance\" between parametric distributions.\n",
        "\n",
        "Lastly, we look at another means to perform optimization, but different from previous approaches, we show how to generalize notions of \"steepest\" to consider the underlying geometry in a distributional sense through *natural* gradient descent."
      ],
      "metadata": {
        "id": "4R7VI43gT937"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Families\n",
        "Let $\\eta = [\\eta_1, \\dotsc, \\eta_k]$ be a $k$-vector of parameters, and $x$ be an observation such that $x \\sim f(\\eta)$. We can define its [PDF](https://en.wikipedia.org/wiki/Probability_density_function) (or [PMF](https://en.wikipedia.org/wiki/Probability_mass_function) in case of discrete $x$) as $$f(x | \\eta) = h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)),$$ where $h(x)$ is a *base measure*, $\\eta$ are the *natural parameters*, $T(x)$ are the [*sufficient statistics*](https://en.wikipedia.org/wiki/Sufficient_statistic), and $A(\\eta)$ is the [*log-partition function*](https://en.wikipedia.org/wiki/Partition_function_(mathematics)). If $\\eta$ is *finite*, and the [*support*](https://en.wikipedia.org/wiki/Support_(mathematics)#In_probability_and_measure_theory) of $f$ does not depend on the value of $\\eta$, then $f$ can be said to be a member of the [Exponential Families](https://en.wikipedia.org/wiki/Exponential_family).\n",
        "\n",
        "### Example: Normal Distribution\n",
        "Recall if $x \\sim N(\\mu, \\sigma^2)$, then the PDF of $x$ is given by,\n",
        "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right).$$ To see that the two-parameter Normal distribution is a member of the Exponential Families, define $\\eta = [\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}]$, $h(x) = \\frac{1}{\\sqrt{2\\pi}}$, $T(x) = [x, x^2]^T$, and $A(\\eta) = \\frac{\\mu^2}{2\\sigma^2} + \\log |\\sigma| = -\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log|\\frac{1}{\\eta_2}|$. Placing this all together we have,\n",
        "$$\\begin{align*}\n",
        "f(x | \\eta) &= h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot T(x) - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot [x, x^2]^T - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} + \\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2}\\log\\left|\\frac{1}{\\eta_2}\\right|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "## Expectations\n",
        "A key property of ExpFam distributions is that one can define the moments of $T(x)$ from the log-partition function $A(\\eta)$. Two key moments to remember are, $\\mathbb{E}[T(x)] = \\frac{\\partial}{\\partial \\eta} A(\\eta)$ and $\\mathbb{V}[T(x)] = \\frac{\\partial^2}{\\partial \\eta \\partial \\eta^T} A(\\eta)$. In the case of scalar Normal variables, we have\n",
        "$$\\begin{align*}\n",
        "\\mathbb{E}[T(x)] &= \\frac{\\partial}{\\partial \\eta} A(\\eta) \\\\\n",
        "  &= \\dotsc \\\\\n",
        "\\mathbb{V}[T(x)] &= \\frac{\\partial^2}{\\partial \\eta \\partial \\eta^T} A(\\eta) \\\\\n",
        "  &= \\dotsc \\\\ .\n",
        "\\end{align*}$$\n",
        "\n",
        "## Inference\n",
        "TBD"
      ],
      "metadata": {
        "id": "tz3bA7fUcApH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppu0j4ydwTnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Divergences\n",
        "[Divergences](https://en.wikipedia.org/wiki/Divergence_(statistics)) capture a notion of \"[statistical distance](https://en.wikipedia.org/wiki/Statistical_distance)\" between parameterized distribution functions. Their full definition is beyond the scope of this course, but key properties to recall are, given distributions $q$ and $p$ over the same [sample space](https://en.wikipedia.org/wiki/Sample_space), a divergence statisfies $D(p || q) \\geq 0$ and $D(p || q) = 0$ iff $p = q$. There are many [different kinds](https://en.wikipedia.org/wiki/Divergence_(statistics)#Examples) of divergences, but a very commonly used divergence is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), or KL divergence.\n",
        "\n",
        "For discrete $x$ with sample space $\\mathcal{X}$, we have, $$D_{KL}(p || q) = \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)} = - \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{q(x)}{p(x)}.$$\n",
        "\n",
        "Fo continuous $x \\in \\mathbb{R}$, we have, $$D_{KL}(p || q) = \\int_{-\\infty}^\\infty p(x) \\log \\frac{p(x)}{q(x)}dx = -\\int_{-\\infty}^\\infty p(x) \\log \\frac{q(x)}{p(x)}dx.$$\n",
        "\n",
        "## Example: Normal Distribution\n",
        "Let $p := N(\\mu_p, \\sigma^2_p)$ and $q := N(\\mu_q, \\sigma^2_q).$ The KL divergence between $p, q$ is given by,\n",
        "$$D_{KL}(p || q) = \\frac{(\\mu_p - \\mu_q)^2}{2 \\sigma^2_q} + \\frac{1}{2}\\left(\\frac{\\sigma^2_p}{\\sigma^2_q} - 1 - \\ln \\frac{\\sigma^2_p}{\\sigma^2_q} \\right).$$"
      ],
      "metadata": {
        "id": "6joftcZub-KC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kja5ci2qTN0X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Gradient Descent\n",
        "Recall under [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) we can iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n",
        "$$ \\theta_{t+1} = \\theta_t - \\rho_t \\nabla f(\\theta_t).$$ We can re-write this update as,\n",
        "$$ \\theta_{t+1} = {\\arg \\min}_{\\theta} \\ \\ \\theta \\cdot \\nabla f(\\theta_t) - \\frac{1}{\\rho_t}||\\theta - \\theta_t||^2.$$ To see their equivalence we have,\n",
        "$$\\begin{align*}\n",
        "TBD\n",
        "\\end{align*}$$\n",
        "\n",
        "What if rather than considering *Euclidean distance* we used a notion of *statistical distance* or *divergence*?"
      ],
      "metadata": {
        "id": "9Yh-wOPnwU9m"
      }
    }
  ]
}