{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1HBzlIfS/1hAjfO3SrRuP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab_6_ExpFam_Divergences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It's a Family Affair, or: Exponential Families, Natural Gradient Descent, & Statistical Divergences\n",
        "\n",
        "[Exponential Families](https://en.wikipedia.org/wiki/Exponential_family) (sometimes abbreviated as ExpFam) provide a [succinct characterization](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) of many distributions (e.g., [Normal](https://en.wikipedia.org/wiki/Normal_distribution), [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution), [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), [Wishart](https://en.wikipedia.org/wiki/Wishart_distribution), etc.). We'll take an informal look at their properties and how to perform inference.\n",
        "\n",
        "We'll also take an informal review of [statistical divergences](https://en.wikipedia.org/wiki/Divergence_(statistics)), which reflect notions of a \"distance\" between parametric distributions.\n",
        "\n",
        "Lastly, we look at another means to perform optimization, but different from previous approaches, we show how to generalize notions of \"steepest\" to consider the underlying geometry in a distributional sense through *natural* gradient descent."
      ],
      "metadata": {
        "id": "4R7VI43gT937"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Families\n",
        "Let $\\eta = [\\eta_1, \\dotsc, \\eta_k]$ be a $k$-vector of parameters, and $x$ be an observation such that $x \\sim f(\\eta)$. We can define its [PDF](https://en.wikipedia.org/wiki/Probability_density_function) (or [PMF](https://en.wikipedia.org/wiki/Probability_mass_function) in case of discrete $x$) as $$f(x | \\eta) = h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)),$$ where $h(x)$ is a *base measure*, $\\eta$ are the *natural parameters*, $T(x)$ are the [*sufficient statistics*](https://en.wikipedia.org/wiki/Sufficient_statistic), and $A(\\eta)$ is the [*log-partition function*](https://en.wikipedia.org/wiki/Partition_function_(mathematics)). If $\\eta$ is *finite*, and the [*support*](https://en.wikipedia.org/wiki/Support_(mathematics)#In_probability_and_measure_theory) of $f$ does not depend on the value of $\\eta$, then $f$ can be said to be a member of the [Exponential Families](https://en.wikipedia.org/wiki/Exponential_family).\n",
        "\n",
        "### Example: Normal Distribution\n",
        "Recall if $x \\sim N(\\mu, \\sigma^2)$, then the PDF of $x$ is given by,\n",
        "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right).$$ To see that the two-parameter Normal distribution is a member of the Exponential Families, define $\\eta = [\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}]$, $h(x) = \\frac{1}{\\sqrt{2\\pi}}$, $T(x) = [x, x^2]^T$, and $A(\\eta) = \\frac{\\mu^2}{2\\sigma^2} + \\log |\\sigma| = -\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log|\\frac{1}{\\eta_2}|$. Placing this all together we have,\n",
        "$$\\begin{align*}\n",
        "f(x | \\eta) &= h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot T(x) - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot [x, x^2]^T - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} + \\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2}\\log\\left|\\frac{1}{\\eta_2}\\right|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "## Expectations\n",
        "A key property of ExpFam distributions is that one can define the moments of $T(x)$ from the log-partition function $A(\\eta)$. Two key moments to remember are, $\\mathbb{E}[T(x)] = \\frac{\\partial}{\\partial \\eta} A(\\eta)$ and $\\mathbb{V}[T(x)] = \\frac{\\partial^2}{\\partial \\eta \\partial \\eta^T} A(\\eta)$.\n",
        "\n",
        "In the case of scalar Normal variables, we have $\\mathbb{E}[T(x)] = \\mathbb{E}\\left[[x, x^2]\\right] = [\\mu, \\sigma^2 + \\mu^2]$ and $\\mathbb{V}[T(x)] = [\\sigma^2, \\sigma^4]$.\n",
        "$$\\begin{align*}\n",
        "\\mathbb{E}[T(x)] &= \\frac{\\partial}{\\partial \\eta} A(\\eta) \\\\\n",
        "  &= \\frac{\\partial}{\\partial \\eta} \\left[-\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log|\\frac{1}{\\eta_2}|\\right] \\\\\n",
        "  &= -\\frac{\\partial}{\\partial \\eta} \\frac{\\eta_1^2}{4\\eta_2} + \\frac{\\partial}{\\partial \\eta}\\frac{1}{2}\\log|\\frac{1}{\\eta_2}| \\\\\n",
        "  &= -\\frac{\\partial}{\\partial \\eta} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta}\\frac{1}{2}\\log|\\eta_2| \\\\\n",
        "\\mathbb{E}[T(x)]_1 &= -\\frac{\\partial}{\\partial \\eta_1} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta_1}\\frac{1}{2}\\log|\\eta_2| \\\\\n",
        "  &= - \\frac{2 \\eta_1}{4 \\eta_2} = -\\frac{\\eta_1}{2 \\eta_2} = -\\frac{\\mu / \\sigma^2}{-2 / (2\\sigma^2)} = \\mu \\\\\n",
        "\\mathbb{E}[T(x)]_2 &= -\\frac{\\partial}{\\partial \\eta_2} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta_2}\\frac{1}{2}\\log|\\eta_2| \\\\\n",
        "  &= \\frac{\\eta^2_1}{\\eta_2^2} - \\frac{1}{2\\eta_2} = \\frac{\\mu^2/\\sigma^4}{1/\\sigma^4} + \\frac{1}{1 / \\sigma^2} = \\mu^2 + \\sigma^2\\\\\n",
        "\\mathbb{V}[T(x)] &= \\frac{\\partial^2}{\\partial \\eta \\partial \\eta^T} A(\\eta) \\\\\n",
        "  &= \\text{Ha, ha! For the HW} .\n",
        "\\end{align*}$$\n",
        "\n",
        "## Inference\n",
        "Given $x_1, \\dotsc, x_n$ we assume that $x_i \\sim f(\\eta)$ independently and identically distributed under some ExpFam dist $f$ with natural parameters $\\eta$.\n",
        "\n",
        "Our log-likelihood is then given by,\n",
        "$$ \\ell(\\eta) = \\sum_{i=1}^n \\log f(x_i | \\eta).$$ To identify the\n",
        "maximum likelihood estimates we must first compute the gradient wrt $\\eta$, which is given by\n",
        "$$\\begin{align*}\n",
        "\\nabla \\ell(\\eta) &= \\sum_{i=1}^n \\nabla \\log f(x_i | \\eta) \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\log \\left[ h(x_i)\\exp(\\eta \\cdot T(x_i) - A(\\eta))\\right] \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\left[\\log h(x_i) + \\log \\exp(\\eta \\cdot T(x_i) - A(\\eta))\\right] \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\log h(x_i) + \\nabla \\log \\exp(\\eta \\cdot T(x_i) - A(\\eta)) \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla [\\eta \\cdot T(x_i)] - \\nabla A(\\eta) \\\\\n",
        "  &= \\sum_{i=1}^n T(x_i) - \\mathbb{E}[T(x_i)] ⇒ \\\\\n",
        "\\end{align*}$$\n",
        "We would like to find values $\\eta$ such that $\\sum_i T(x_i) = \\sum_i \\mathbb{E}[T(x_i)] = n \\mathbb{E}[T(X)]$."
      ],
      "metadata": {
        "id": "tz3bA7fUcApH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykn-OzCv3xjQ",
        "outputId": "4b688546-1729-432e-af81-acd5ad6e0f7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting equinox\n",
            "  Downloading equinox-0.11.3-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from equinox) (0.4.23)\n",
            "Collecting jaxtyping>=0.2.20 (from equinox)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from equinox) (4.9.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (1.11.4)\n",
            "Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.20->equinox)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox\n",
            "Successfully installed equinox-0.11.3 jaxtyping-0.2.25 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "\n",
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxtyping import Array, ArrayLike\n",
        "\n",
        "\n",
        "class ExpFam(eqx.Module):\n",
        "  x: Array\n",
        "\n",
        "  @abstractmethod\n",
        "  def base_measure(self) -> Array:\n",
        "    ...\n",
        "\n",
        "  @abstractmethod\n",
        "  def sufficient_statistics(self) -> Array:\n",
        "    ...\n",
        "\n",
        "  @abstractmethod\n",
        "  def log_partition(self, eta: ArrayLike) -> Array:\n",
        "    ...\n",
        "\n",
        "  def loglikelihood(self, eta: ArrayLike) -> Array:\n",
        "    t_x = self.sufficient_statistics(self.x)\n",
        "    log_h_x = jnp.log(self.base_measure(self.x))\n",
        "    log_eta = self.log_partition(eta)\n",
        "    return t_x @ eta - log_eta + log_h_x\n",
        "\n",
        "  def __call__(self, eta: ArrayLike) -> Array:\n",
        "    return self.loglikelihood(eta)\n",
        "\n",
        "\n",
        "class Normal(ExpFam):\n",
        "\n",
        "  def base_measure(self) -> Array:\n",
        "    return 1. / jnp.sqrt(2 * jnp.pi)\n"
      ],
      "metadata": {
        "id": "ppu0j4ydwTnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Divergences\n",
        "[Divergences](https://en.wikipedia.org/wiki/Divergence_(statistics)) capture a notion of \"[statistical distance](https://en.wikipedia.org/wiki/Statistical_distance)\" between parameterized distribution functions. Their full definition is beyond the scope of this course, but key properties to recall are, given distributions $q$ and $p$ over the same [sample space](https://en.wikipedia.org/wiki/Sample_space), a divergence statisfies $D(p || q) \\geq 0$ and $D(p || q) = 0$ iff $p = q$. There are many [different kinds](https://en.wikipedia.org/wiki/Divergence_(statistics)#Examples) of divergences, but a very commonly used divergence is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), or KL divergence.\n",
        "\n",
        "For discrete $x$ with sample space $\\mathcal{X}$, we have, $$D_{KL}(p || q) = \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)} = - \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{q(x)}{p(x)}.$$\n",
        "\n",
        "Fo continuous $x \\in \\mathbb{R}$, we have, $$D_{KL}(p || q) = \\int_{-\\infty}^\\infty p(x) \\log \\frac{p(x)}{q(x)}dx = -\\int_{-\\infty}^\\infty p(x) \\log \\frac{q(x)}{p(x)}dx.$$\n",
        "\n",
        "## Example: Normal Distribution\n",
        "Let $p := N(\\mu_p, \\sigma^2_p)$ and $q := N(\\mu_q, \\sigma^2_q).$ The KL divergence between $p, q$ is given by,\n",
        "$$D_{KL}(p || q) = \\frac{(\\mu_p - \\mu_q)^2}{2 \\sigma^2_q} + \\frac{1}{2}\\left(\\frac{\\sigma^2_p}{\\sigma^2_q} - 1 - \\ln \\frac{\\sigma^2_p}{\\sigma^2_q} \\right).$$"
      ],
      "metadata": {
        "id": "6joftcZub-KC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kja5ci2qTN0X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Gradient Descent\n",
        "Recall under [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) we can iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n",
        "$$ \\theta_{t+1} = \\theta_t - \\rho_t \\nabla f(\\theta_t).$$ We can re-write this update as,\n",
        "$$ \\theta_{t+1} = {\\arg \\min}_{\\theta} \\ \\ \\theta \\cdot \\nabla f(\\theta_t) - \\frac{1}{\\rho_t}||\\theta - \\theta_t||^2.$$ To see their equivalence we have,\n",
        "$$\\begin{align*}\n",
        "TBD\n",
        "\\end{align*}$$\n",
        "\n",
        "What if rather than considering *Euclidean distance* we used a notion of *statistical distance* or *divergence*?"
      ],
      "metadata": {
        "id": "9Yh-wOPnwU9m"
      }
    }
  ]
}