{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab_2_LinearSolve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J01SPGCfMEma"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jnpla\n",
    "import jax.random as rdm\n",
    "import jax.scipy.linalg as jspla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO0OrFtLOxoA"
   },
   "source": [
    "## Sum(mer) Madness, or Solving Linear Equations\n",
    "Or how I learned not to take the inverse and perform [matrix-vector products](https://en.wikipedia.org/wiki/Operator_(mathematics)).\n",
    "\n",
    "Given $n \\times n$ non-singular (i.e. \"nice\") matrix $A$, $n \\times 1$ column-vector $b$, we can describe a [linear system of equations](https://en.wikipedia.org/wiki/System_of_linear_equations#General_form) related $A$, $b$ as $$Ax=b.$$\n",
    "Algebraically, we can solve for $x$ as, $x = A^{-1}b$, but how should we go about this [_numerically_](https://en.wikipedia.org/wiki/Numerical_linear_algebra)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp93hPA4O-QL",
    "outputId": "e5cd49db-38d2-4ae7-c6cd-a6b0fa9caf1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct dist = 0.0003477725840639323 | solve_dist = 0.0002090586203848943\n"
     ]
    }
   ],
   "source": [
    "def sim_linear_system(key, n: int):\n",
    "  key, x_key = rdm.split(key)\n",
    "  A = rdm.normal(key, shape=(n,n))\n",
    "  x = rdm.normal(x_key, shape=(n,))\n",
    "  b = A @ x\n",
    "  return A, x, b\n",
    "\n",
    "seed = 0\n",
    "N = 100\n",
    "\n",
    "key = rdm.PRNGKey(seed)\n",
    "key, sim_key = rdm.split(key)\n",
    "A, x, b = sim_linear_system(sim_key, N)\n",
    "\n",
    "# solve using algebraic approach\n",
    "Ainv = jnpla.inv(A)\n",
    "x_hat_direct = Ainv @ b\n",
    "\n",
    "# solve using 'blackbox(!?)' solver\n",
    "x_hat = jnpla.solve(A, b)\n",
    "\n",
    "# measure distance from truth using 2norm\n",
    "direct_dist = jnpla.norm(x - x_hat_direct)\n",
    "solve_dist = jnpla.norm(x - x_hat)\n",
    "\n",
    "print(f\"direct dist = {direct_dist} | solve_dist = {solve_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oasiBAY7R3Tj"
   },
   "source": [
    "We see less [numerical error](https://en.wikipedia.org/wiki/Numerical_analysis#Numerical_stability_and_well-posed_problems) in the solution using the 'solve' approach compared to our attempt at using the 'algebraic' solution. What happens as `N` increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EoUXCHaiSH0S",
    "outputId": "d3ffe3b6-5c37-4a42-9310-60efb466d183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 100 | direct dist = 0.0002750760759226978 | solve_dist = 4.570868986775167e-05\n",
      "N = 1000 | direct dist = 0.02133580483496189 | solve_dist = 0.003495120210573077\n",
      "N = 5000 | direct dist = 0.3149307668209076 | solve_dist = 0.04819103330373764\n"
     ]
    }
   ],
   "source": [
    "for N in [100, 1000, 5000]:\n",
    "\n",
    "  key, sim_key = rdm.split(key)\n",
    "  A, x, b = sim_linear_system(sim_key, N)\n",
    "\n",
    "  # solve using algebraic approach\n",
    "  Ainv = jnpla.inv(A)\n",
    "  x_hat_direct = Ainv @ b\n",
    "\n",
    "  # solve using 'blackbox(!?)' solver\n",
    "  x_hat = jnpla.solve(A, b)\n",
    "\n",
    "  # measure distance from truth using 2norm\n",
    "  direct_dist = jnpla.norm(x - x_hat_direct)\n",
    "  solve_dist = jnpla.norm(x - x_hat)\n",
    "\n",
    "  print(f\"N = {N} | direct dist = {direct_dist} | solve_dist = {solve_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n7nGU4VnoCc"
   },
   "source": [
    "### How is `jnpla.solve` _solving_ the system?\n",
    "\n",
    "Numerical solvers typically perform some _decomopsition_ of $A$ into a product of structured matrices (e.g., orthogonal, lower/upper diagonal, permutations, etc.) which permit direct solutions through forward/backward solvers. A couple prominant examples are given below.\n",
    "\n",
    "[QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition#Using_for_solution_to_linear_inverse_problems)\n",
    "\n",
    "\n",
    "$A = QR$ where $Q$ is $n \\times n$ [orthonormal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) (i.e. $Q Q' = Q'Q = I$, thus $Q^{-1} = Q'$) and $R$ is $n \\times n$ upper [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix) (i.e. $R_{ij} = 0, i < j$).\n",
    "\n",
    "Solving $Ax = b$ using QR amounts to noticing,\n",
    "$$QRx = b ⇒ Rx = Q^{-1}b = Q'b,$$ which can then be solved using a backwards solve. See, [jax.numpy.linalg.qr](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linalg.qr.html), [jax.scipy.linalg.solve_triangular](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.linalg.solve_triangular.html) for reference.\n",
    "\n",
    "[LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition#Solving_linear_equations)\n",
    "\n",
    "$A = PLU$ where $P$ is an $n \\times n$ [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix), $L$ is $n \\times n$ lower triangular matrix and $U$ is an $n \\times n$ upper triangular matrix. Solving $Ax = b$ using LU amounts to, $$PLU = b ⇒ LU = P^{-1}b = P'b,$$ which can be solved with one forwards solve $Ly = P'b$ and then a backwards solve $Ux = y$. See, [jax.scipy.linalg.lu_factor](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.linalg.lu_factor.html), [jax.scipy.linalg.lu_solve](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.linalg.lu_solve.html) for reference.\n",
    "\n",
    "Let's try to assess which is used by `jnpla.solve` by comparing error of the solutions (not ideal, but not worst idea possible; see [jax.numpy.linalg.solve](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linalg.solve.html) for reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr1IIFXjS44u",
    "outputId": "d40f02c8-2c48-46b6-bb27-bb435ed399a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct dist = 2.836631210811902e-05 | qr dist = 2.3936559955473058e-05 | lu dist = 4.185290890745819e-06 | solve_dist = 4.185290890745819e-06\n"
     ]
    }
   ],
   "source": [
    "N = 25\n",
    "key, sim_key = rdm.split(key)\n",
    "A, x, b = sim_linear_system(sim_key, N)\n",
    "\n",
    "# solve using algebraic approach\n",
    "Ainv = jnpla.inv(A)\n",
    "x_direct = Ainv @ b\n",
    "\n",
    "# QR\n",
    "Q, R = jnpla.qr(A)\n",
    "x_qr = jspla.solve_triangular(R, Q.T @ b)\n",
    "\n",
    "# LU\n",
    "LU, P = jspla.lu_factor(A)\n",
    "x_lu = jspla.lu_solve((LU, P), b)\n",
    "\n",
    "# direct solve for baseline\n",
    "x_solve = jnpla.solve(A, b)\n",
    "\n",
    "direct_dist = jnpla.norm(x - x_direct)\n",
    "qr_dist = jnpla.norm(x - x_qr)\n",
    "lu_dist = jnpla.norm(x - x_lu)\n",
    "solve_dist = jnpla.norm(x - x_solve)\n",
    "\n",
    "\n",
    "print(f\"direct dist = {direct_dist} | qr dist = {qr_dist} | lu dist = {lu_dist} | solve_dist = {solve_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCM1v4ATZlC8"
   },
   "source": [
    "### How much time does it take to solve a system of linear equations?\n",
    "Complexity is $O(n^3)$ when $A$ is $n \\times n$ (under exact numerical precision, i.e. ignoring bit complexity of numerical results), but can we do better in terms of constants? Pls no [mention](https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication#Matrix_multiplication_exponent) of $O(n^\\omega)$.\n",
    "\n",
    "What if we know that our matrix $A$ has additional structure from the start?\n",
    "\n",
    "[Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition)\n",
    "\n",
    "When $A$ is an $n \\times n$ [_positive definite_](https://en.wikipedia.org/wiki/Definite_matrix) (i.e. $x'Ax \\geq 0, ∀ x \\in R^n$), the Cholesky decomposition gives $A = LL'$ where $L$ is an $n \\times n$ lower triangular matrix.\n",
    "\n",
    "Solving $Ax=b$ using Cholesky decomposition amounts to, $$Ax=b ⇒LL'x = b,$$\n",
    "which  can be solved with one forwards solve $Ly=b$ and then a backwards solve $L'x=y$. This will be roughly twice as fast compared with LU decomposition approaches. See [jax.scipy.linalg.cho_factor](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.linalg.cho_factor.html) and [jax.scipy.linalg.cho_solve](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.linalg.cho_solve.html) for reference.\n",
    "\n",
    "Let's compare the error of the solve solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68jTTKDDCt7O",
    "outputId": "9530755d-868e-4e4e-e8fb-63724e577a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct dist = 2.7083056920673698e-05 | qr dist = 2.6027397325378843e-05 | cho dist = 1.7303784261457622e-05 | solve_dist = 1.732515011099167e-05\n"
     ]
    }
   ],
   "source": [
    "def sim_sym_linear_system(key, n: int):\n",
    "  key, x_key = rdm.split(key, 2)\n",
    "  # sample larger row to improve conditioning\n",
    "  A = rdm.normal(key, shape=(2*n,n))\n",
    "  # rescale back to n x n\n",
    "  A = A.T @ A\n",
    "  x = rdm.normal(x_key, shape=(n,))\n",
    "  b = A @ x\n",
    "  return A, x, b\n",
    "\n",
    "key, sim_key = rdm.split(key)\n",
    "N = 500\n",
    "\n",
    "A, x, b = sim_sym_linear_system(sim_key, N)\n",
    "\n",
    "# solve using algebraic approach\n",
    "Ainv = jnpla.inv(A)\n",
    "x_direct = Ainv @ b\n",
    "\n",
    "# QR\n",
    "Q, R = jnpla.qr(A)\n",
    "x_qr = jspla.solve_triangular(R, Q.T @ b)\n",
    "\n",
    "# Cholesky\n",
    "L, lower = jspla.cho_factor(A)\n",
    "x_cho = jspla.cho_solve((L, lower), b)\n",
    "\n",
    "# direct solve for baseline\n",
    "x_solve = jnpla.solve(A, b)\n",
    "\n",
    "direct_dist = jnpla.norm(x - x_direct)\n",
    "qr_dist = jnpla.norm(x - x_qr)\n",
    "cho_dist = jnpla.norm(x - x_cho)\n",
    "solve_dist = jnpla.norm(x - x_solve)\n",
    "\n",
    "print(f\"direct dist = {direct_dist} | qr dist = {qr_dist} | cho dist = {cho_dist} | solve_dist = {solve_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDtGoVs_GyeP"
   },
   "source": [
    "### LAB portion\n",
    "\n",
    "What is the average runtime for each of these operations?\n",
    "Hint: use the `%timeit` magic command and [`block_until_ready`](https://jax.readthedocs.io/en/latest/_autosummary/jax.block_until_ready.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v65o0jsHG1Rt",
    "outputId": "de588426-3821-4cdd-e46c-6187ef9bd632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 ms ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "key, sim_key = rdm.split(key)\n",
    "A, x, b = sim_sym_linear_system(sim_key, N)\n",
    "\n",
    "# solve using algebraic approach\n",
    "def _direct(A, b):\n",
    "  pass\n",
    "\n",
    "# QR\n",
    "def _qr(A, b):\n",
    "  pass\n",
    "\n",
    "# Cholesky\n",
    "def _cholesky(A, b):\n",
    "  pass\n",
    "\n",
    "# general solver (uses LU)\n",
    "def _solve(A, b):\n",
    "  return jnpla.solve(A, b)\n",
    "\n",
    "%timeit x_solve = _solve(A, b).block_until_ready()\n",
    "%timeit x_direct = _direct(A, b).block_until_ready()\n",
    "%timeit x_qr = _qr(A, b).block_until_ready()\n",
    "%timeit x_cho = _cholesky(A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ORvGCONKJV8"
   },
   "source": [
    "Great! Cholesky is the fastest and has error on par with the general `solve` command. What can we gain in terms of speed by using [`jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcf4ynyAKSYY"
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "key, sim_key = rdm.split(key)\n",
    "A, x, b = sim_sym_linear_system(sim_key, N)\n",
    "\n",
    "jit_direct = jax.jit(_direct)\n",
    "jit_qr = jax.jit(_qr)\n",
    "jit_cholesky = jax.jit(_cholesky)\n",
    "jit_solve = jax.jit(jnpla.solve)\n",
    "\n",
    "%timeit x_direct = jit_direct(A, b).block_until_ready()\n",
    "%timeit x_qr = jit_qr(A, b).block_until_ready()\n",
    "%timeit x_cho = jit_cholesky(A, b).block_until_ready()\n",
    "%timeit x_solve = jit_solve(A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1ZgjnttNJqv"
   },
   "source": [
    "Not much improvement! What gives?\n",
    "\n",
    "Turns out, JAX is already JIT compiling these lower level functions (e.g., solve, decompositions) and our code is not much larger in scope, thus benefit not clearly demonstrated. The takeaway here is not that JIT doesn't help, but rather, to get the most out of JIT'd code, we should apply it to larger programs/functions, which can better optimize the computational graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgRH20aEqdAt"
   },
   "source": [
    "## Iterative Solvers\n",
    "So far we used direct factorizations (QR/LU/Cholesky), which are usually reliable for dense systems but cost about $O(n^3)$ time and $O(n^2)$ memory. For larger or structured problems, [iterative methods](https://en.wikipedia.org/wiki/Iterative_method) can be more efficient because they only require repeated [matrix-vector products](https://en.wikipedia.org/wiki/Matrix_multiplication), not an explicit factorization.\n",
    "\n",
    "To see how this arises, a useful way to understand linear solves is through optimization. Assume $A \\in \\mathbb{R}^{n\\times n}$ is symmetric positive-definite, and define the quadratic\n",
    "$$\n",
    "f(x) = \\tfrac12 x^\\top A x - b^\\top x + c.\n",
    "$$\n",
    "Its gradient is\n",
    "$$\n",
    "\\nabla f(x) = Ax - b.\n",
    "$$\n",
    "So the minimizer $x^\\star$ satisfies $\\nabla f(x^\\star)=0$, which is exactly the linear system\n",
    "$$\n",
    "Ax^\\star = b.\n",
    "$$\n",
    "\n",
    "This gives intuition for iterative methods: at iterate $x_k$, the residual\n",
    "$$\n",
    "r_k = b - Ax_k = -\\nabla f(x_k)\n",
    "$$\n",
    "is the negative gradient of the quadratic objective. If $r_k \\neq 0$, we can decrease $f$ with updates\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k p_k,\n",
    "$$\n",
    "for a search direction $p_k$ and step size $\\alpha_k$.\n",
    "\n",
    "A standard example is [Conjugate Gradient (CG)](https://en.wikipedia.org/wiki/Conjugate_gradient_method), a [Krylov subspace method](https://en.wikipedia.org/wiki/Krylov_subspace) for symmetric positive-definite systems. CG chooses directions that are $A$-conjugate and searches in\n",
    "$$\n",
    "x_k \\in x_0 + \\mathcal{K}_k(A,r_0), \\quad \\mathcal{K}_k(A,r_0)=\\mathrm{span}\\{r_0,Ar_0,\\dots,A^{k-1}r_0\\}.\n",
    "$$\n",
    "That is why we can solve $Ax=b$ using repeated [matrix-vector products](https://en.wikipedia.org/wiki/Matrix_multiplication#Matrix_vector_multiplication), without explicitly forming $A^{-1}$ or a dense factorization.\n",
    "\n",
    "In exact arithmetic, CG converges in at most $n$ iterations; in practice we stop once $\\|r_k\\|/\\|b\\|$ is below tolerance (for example via [`jax.scipy.sparse.linalg.cg`](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.sparse.linalg.cg.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB7C0NUkqb7j"
   },
   "source": [
    "## And now for something completely different... [Lineax](https://docs.kidger.site/lineax/)\n",
    "Library for solving linear systems and related linear computational tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvHjwTYmNE-F",
    "outputId": "082ac1a8-3f0a-4165-b2ec-1f6609b24394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lineax in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
      "Requirement already satisfied: equinox>=0.11.5 in /usr/local/lib/python3.11/dist-packages (from lineax) (0.11.11)\n",
      "Requirement already satisfied: jax>=0.4.26 in /usr/local/lib/python3.11/dist-packages (from lineax) (0.5.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.20 in /usr/local/lib/python3.11/dist-packages (from lineax) (0.2.36)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from lineax) (4.12.2)\n",
      "Requirement already satisfied: jaxlib<=0.5.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (0.5.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (1.26.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lineax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0amWH5fNOYqA",
    "outputId": "28ec0727-6dc4-4ddd-a299-a106aee42892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist lineax solve = 2.793022031255532e-05\n",
      "psd lineax solve = 3.014422873093281e-05\n",
      "cg lineax solve = 3.1980012863641605e-05\n",
      "jax solve = 2.793022031255532e-05\n"
     ]
    }
   ],
   "source": [
    "import lineax as lx\n",
    "\n",
    "N = 1000\n",
    "key, sim_key = rdm.split(key)\n",
    "A, x, b = sim_sym_linear_system(sim_key, N)\n",
    "\n",
    "A_op = lx.MatrixLinearOperator(A)\n",
    "sol = lx.linear_solve(A_op, b)\n",
    "x_sol = sol.value\n",
    "\n",
    "A_psd_op = lx.MatrixLinearOperator(A, lx.positive_semidefinite_tag)\n",
    "psd_sol = lx.linear_solve(A_psd_op, b)\n",
    "x_psd_sol = psd_sol.value\n",
    "\n",
    "# initialize our CG solver\n",
    "cg_solver = lx.CG(atol=1e-5, rtol=1e-4)\n",
    "y0 = b / jnp.diag(A)\n",
    "cg_sol = lx.linear_solve(A_psd_op, b, solver=cg_solver, options={\"y0\":y0})\n",
    "x_cg_sol = cg_sol.value\n",
    "\n",
    "x_solve = jnpla.solve(A, b)\n",
    "\n",
    "dist_sol = jnpla.norm(x - x_sol)\n",
    "dist_psd = jnpla.norm(x - x_psd_sol)\n",
    "dist_cg = jnpla.norm(x - x_cg_sol)\n",
    "dist_solve = jnpla.norm(x - x_solve)\n",
    "\n",
    "print(f\"dist lineax solve = {dist_sol}\")\n",
    "print(f\"psd lineax solve = {dist_psd}\")\n",
    "print(f\"cg lineax solve = {dist_cg}\")\n",
    "print(f\"jax solve = {dist_solve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbVsbI7Bx0Dr"
   },
   "source": [
    "Let's look at slightly more sophisticated linear operator. Let $A = B B' + I_n$ we want to solve $Ax = b$, where $B$ is shape $n \\times k$ for $k < n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChXNObUUyQlF",
    "outputId": "10eea4cf-5da0-410c-e423-e3e76c783bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist lineax solve = 0.003705778392031789\n",
      "cg lineax solve = 0.0006713042384944856\n"
     ]
    }
   ],
   "source": [
    "N, K = 1000, 50\n",
    "key, b_key, x_key = rdm.split(key, 3)\n",
    "\n",
    "B = rdm.normal(b_key, shape=(N, K))\n",
    "x = rdm.normal(x_key, shape=(N,))\n",
    "b = B @ (B.T @ x) + x\n",
    "\n",
    "B_op = lx.MatrixLinearOperator(B)\n",
    "I_op = lx.IdentityLinearOperator(jax.eval_shape(lambda: x))\n",
    "A_op = lx.TaggedLinearOperator(B_op @ B_op.T + I_op, lx.positive_semidefinite_tag)\n",
    "\n",
    "sol = lx.linear_solve(A_op, b)\n",
    "cg_sol = lx.linear_solve(A_op, b, solver=cg_solver)\n",
    "\n",
    "dist_sol = jnpla.norm(x - sol.value)\n",
    "dist_cg = jnpla.norm(x - cg_sol.value)\n",
    "print(f\"dist lineax solve = {dist_sol}\")\n",
    "print(f\"cg lineax solve = {dist_cg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntO14u_70qoi",
    "outputId": "f9d40c4a-f0cb-497e-940b-4e4abb6e34ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.99 ms ± 48.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lx.linear_solve(A_op, b).value.block_until_ready()\n",
    "%timeit lx.linear_solve(A_op, b, solver=cg_solver).value.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK54mdzzSKpH",
    "outputId": "2a914c6d-e4e5-4254-ed2d-1b7c3a95dd43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XtX Xty = 4.768477916717529 | X y = 4.768476486206055 | XtX Xty #2 = 4.768477916717529 | XtX Xty CG = 4.768479347229004\n"
     ]
    }
   ],
   "source": [
    "def sim_linear_reg(key, N, P, r2=0.5):\n",
    "\n",
    "  key, x_key = rdm.split(key)\n",
    "  X = rdm.normal(x_key, shape=(N, P))\n",
    "\n",
    "  key, b_key = rdm.split(key)\n",
    "  beta = rdm.normal(b_key, shape=(P,))\n",
    "\n",
    "  # g = jnp.dot(X, beta)\n",
    "  g = X @ beta\n",
    "  s2g = jnp.var(g)\n",
    "\n",
    "  # back out what s2e is, such that s2g / (s2g + s2e) == h2\n",
    "  s2e = (1 - r2) / r2 * s2g\n",
    "  key, y_key = rdm.split(key)\n",
    "\n",
    "  # add env noise to g, but scale such that var(e) == s2e\n",
    "  y = g + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
    "  return y, X, beta\n",
    "\n",
    "\n",
    "N, P = 1000, 150\n",
    "key, sim_key = rdm.split(key)\n",
    "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
    "\n",
    "XtX = X.T @ X\n",
    "Xty = X.T @ y\n",
    "\n",
    "# dist func\n",
    "def _dist(sol):\n",
    "  return jnpla.norm(beta - sol.value)\n",
    "\n",
    "X_op = lx.MatrixLinearOperator(X)\n",
    "XtX_op = lx.MatrixLinearOperator(XtX, lx.positive_semidefinite_tag)\n",
    "\n",
    "# 1st: linear operator over `XtX` and solve against Xty\n",
    "sol_1 = lx.linear_solve(XtX_op, Xty)\n",
    "sol_1_dist = _dist(sol_1)\n",
    "\n",
    "# 2nd: linear operator over `X` and solve against `y`\n",
    "sol_2 = lx.linear_solve(X_op, y, solver=lx.AutoLinearSolver(well_posed=False))\n",
    "sol_2_dist = _dist(sol_2)\n",
    "\n",
    "# 3rd: linear operator over `X`, then compose with X.T (ie Xop.T @ Xop) solve against Xty\n",
    "sol_3 = lx.linear_solve(X_op.T @ X_op, Xty)\n",
    "sol_3_dist = _dist(sol_3)\n",
    "\n",
    "# 4th: linear operator over `X` then solve against `y` using NormalCG\n",
    "solver = lx.NormalCG(atol=1e-4, rtol=1e-4)\n",
    "sol_4 = lx.linear_solve(X_op, y, solver=solver)\n",
    "sol_4_dist = _dist(sol_4)\n",
    "print(f\" XtX Xty = {sol_1_dist} | X y = {sol_2_dist} | XtX Xty #2 = {sol_3_dist} | XtX Xty CG = {sol_4_dist}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
