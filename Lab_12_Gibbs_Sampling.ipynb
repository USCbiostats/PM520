{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEHyK0lMy+qJiBQq5mQV+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab_12_Gibbs_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Gibbs Sampling is a special case of the MCMC family of algorithms. It is widely used to sample from complex joint distributions when direct sampling is intractable but sampling from the conditional distributions is feasible. The algorithm is especially useful in Bayesian inference where one needs to generate samples from the posterior distribution.\n",
        "\n",
        "## 3.1 Basic Idea\n",
        "\n",
        "Suppose we wish to sample from a joint probability distribution $ p(x_1, x_2, \\ldots, x_n) $. Direct sampling may be infeasible, but if we can easily sample from each conditional distribution:\n",
        "- $ p(x_1 \\mid x_2, \\dots, x_n) $\n",
        "- $ p(x_2 \\mid x_1, x_3, \\dots, x_n) $\n",
        "- $ \\cdots $\n",
        "- $ p(x_n \\mid x_1, \\dots, x_{n-1}) $\n",
        "\n",
        "then Gibbs Sampling offers a practical solution.\n",
        "\n",
        "### 3.2 Algorithm Steps\n",
        "\n",
        "1. **Initialization:** Choose initial values $ x_1^{(0)}, x_2^{(0)}, \\dots, x_n^{(0)} $ (often arbitrarily).\n",
        "\n",
        "2. **Iterative Sampling:** For iteration $ t = 1, 2, \\dots, T $:\n",
        "   - Sample\n",
        "     $$\n",
        "     x_1^{(t)} \\sim p\\Big(x_1 \\mid x_2^{(t-1)}, x_3^{(t-1)}, \\dots, x_n^{(t-1)}\\Big)\n",
        "     $$\n",
        "   - Sample\n",
        "     $$\n",
        "     x_2^{(t)} \\sim p\\Big(x_2 \\mid x_1^{(t)}, x_3^{(t-1)}, \\dots, x_n^{(t-1)}\\Big)\n",
        "     $$\n",
        "   - Continue in this fashion until\n",
        "     $$\n",
        "     x_n^{(t)} \\sim p\\Big(x_n \\mid x_1^{(t)}, x_2^{(t)}, \\dots, x_{n-1}^{(t)}\\Big)\n",
        "     $$\n",
        "\n",
        "3. **Convergence:** Under regularity conditions, the chain $\\{(x_1^{(t)}, \\dots, x_n^{(t)})\\}$ converges to the target distribution $ p(x_1, \\dots, x_n) $.\n",
        "\n",
        "### 3.3 Pseudocode\n",
        "\n",
        "```python\n",
        "# Pseudocode for Gibbs Sampling\n",
        "\n",
        "# Initialize x[1], x[2], ..., x[n]\n",
        "initialize x = [x_1^(0), x_2^(0), ..., x_n^(0)]\n",
        "\n",
        "for t in 1 to T:\n",
        "    x[1] = sample from p(x[1] | x[2]^(t-1), x[3]^(t-1), ..., x[n]^(t-1))\n",
        "    x[2] = sample from p(x[2] | x[1]^(t),   x[3]^(t-1), ..., x[n]^(t-1))\n",
        "    ...\n",
        "    x[n] = sample from p(x[n] | x[1]^(t), x[2]^(t), ..., x[n-1]^(t))\n",
        "```\n"
      ],
      "metadata": {
        "id": "0y0qsx8XiNus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements and Considerations\n",
        "\n",
        "### 4.1 Markov Chains and Stationarity\n",
        "\n",
        "- **Markov Property:** The next state depends only on the current state and not on the previous history.\n",
        "- **Stationary Distribution:** The target distribution $ p(x_1, \\dots, x_n) $ is invariant under the Gibbs sampling transition, meaning that once the chain has converged, successive samples are drawn from this distribution.\n",
        "\n",
        "### 4.2 Detailed Balance and Ergodicity\n",
        "\n",
        "- **Detailed Balance:** For any two states $ X $ and $ Y $, the transition probabilities satisfy:\n",
        "  \\[\n",
        "  p(X) \\, P(X \\to Y) = p(Y) \\, P(Y \\to X)\n",
        "  \\]\n",
        "  ensuring that the chain is reversible.\n",
        "- **Ergodicity:** The chain must be irreducible and aperiodic so that it eventually reaches every part of the state space and the time averages converge to ensemble averages.\n",
        "\n",
        "### 4.3 Convergence Considerations\n",
        "\n",
        "- **Burn-in Period:** An initial number of samples may be discarded to allow the chain to reach its stationary distribution.\n",
        "- **Thinning:** To reduce autocorrelation in the sample chain, one may retain only every $ k $-th sample.\n",
        "- **Diagnostics:** Use trace plots, autocorrelation functions, or convergence tests (e.g., Gelmanâ€“Rubin diagnostics) to assess if the chain has converged."
      ],
      "metadata": {
        "id": "SjJT4IYPjq1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Example: Gibbs Sampling for a Bivariate Normal Distribution\n",
        "\n",
        "To illustrate the method, we consider a simple bivariate normal distribution defined by,\n",
        "$$\n",
        "\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\sim \\mathcal{N} \\Bigg(\\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix}, \\begin{pmatrix} \\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{pmatrix}\\Bigg).\n",
        "$$\n",
        "\n",
        "### 6.1 Deriving the Conditional Distributions\n",
        "\n",
        "For a bivariate normal:\n",
        "- The conditional distribution for $ x $ given $ y $ is:\n",
        "  $$\n",
        "  x \\mid y \\sim \\mathcal{N}\\left(\\mu_x + \\rho \\frac{\\sigma_x}{\\sigma_y}(y - \\mu_y),\\; (1-\\rho^2)\\sigma_x^2\\right)\n",
        "  $$\n",
        "- Likewise, the conditional distribution for $ y $ given $ x $ is:\n",
        "  $$\n",
        "  y \\mid x \\sim \\mathcal{N}\\left(\\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(x - \\mu_x),\\; (1-\\rho^2)\\sigma_y^2\\right)\n",
        "  $$"
      ],
      "metadata": {
        "id": "u4vGD2RUkBC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax, jit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set parameters for the bivariate normal distribution\n",
        "mu_x, mu_y = 0.0, 0.0\n",
        "sigma_x, sigma_y = 1.0, 1.0\n",
        "rho = 0.8\n",
        "n_samples = 5000\n",
        "\n",
        "def gibbs_step(state, key):\n",
        "    \"\"\"\n",
        "    Performs one Gibbs update step.\n",
        "\n",
        "    Parameters:\n",
        "        state: tuple (x, y)\n",
        "        key: PRNGKey for randomness\n",
        "    Returns:\n",
        "        new_state: updated (x, y)\n",
        "    \"\"\"\n",
        "    x, y = state\n",
        "    # Split the key for independent randomness for each conditional sample\n",
        "    key_x, key_y = random.split(key)\n",
        "\n",
        "    # Sample x given y\n",
        "    mu_cond_x = mu_x + rho * (sigma_x / sigma_y) * (y - mu_y)\n",
        "    sigma_cond_x = jnp.sqrt((1 - rho**2) * sigma_x**2)\n",
        "    x_new = mu_cond_x + sigma_cond_x * random.normal(key_x)\n",
        "\n",
        "    # Sample y given the newly sampled x\n",
        "    mu_cond_y = mu_y + rho * (sigma_y / sigma_x) * (x_new - mu_x)\n",
        "    sigma_cond_y = jnp.sqrt((1 - rho**2) * sigma_y**2)\n",
        "    y_new = mu_cond_y + sigma_cond_y * random.normal(key_y)\n",
        "\n",
        "    return (x_new, y_new)\n",
        "\n",
        "@jit\n",
        "def run_gibbs(key, initial_state, n_samples):\n",
        "    \"\"\"\n",
        "    Runs the Gibbs sampler for n_samples iterations using jax.lax.scan.\n",
        "\n",
        "    Parameters:\n",
        "        key: PRNGKey for randomness.\n",
        "        initial_state: tuple (x, y) for initial state.\n",
        "        n_samples: number of samples to generate.\n",
        "    Returns:\n",
        "        states: tuple of arrays (x_samples, y_samples) with shape (n_samples,)\n",
        "    \"\"\"\n",
        "    # Prepare a sequence of keys for the scan\n",
        "    keys = random.split(key, n_samples)\n",
        "\n",
        "    def scan_body(state, key):\n",
        "        new_state = gibbs_step(state, key)\n",
        "        # The output of scan (new state) is collected; here we also output new_state.\n",
        "        return new_state, new_state\n",
        "\n",
        "    final_state, states = lax.scan(scan_body, initial_state, keys)\n",
        "    return states\n",
        "\n",
        "# Set up the PRNG key and initial state\n",
        "seed = 42\n",
        "key = random.PRNGKey(seed)\n",
        "initial_state = (0.0, 0.0)\n",
        "\n",
        "# Run the Gibbs sampler\n",
        "states = run_gibbs(key, initial_state, n_samples)\n",
        "x_samples, y_samples = states  # each is a JAX array of shape (n_samples,)\n",
        "\n",
        "# Transfer arrays from device to host for plotting\n",
        "x_samples = jax.device_get(x_samples)\n",
        "y_samples = jax.device_get(y_samples)\n",
        "\n",
        "# Plot the samples\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x_samples, y_samples, alpha=0.3, s=10)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Gibbs Sampling: Bivariate Normal Samples (BlackJAX Style)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Ak9WY4ZkOZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages and Limitations\n",
        "\n",
        "### Advantages\n",
        "- **Simplicity:** Straightforward to implement when the full conditional distributions are known.\n",
        "- **Efficiency:** Can be very effective in high-dimensional settings with conditional independence structures.\n",
        "- **Theoretical Guarantees:** Under proper conditions, convergence to the target distribution is guaranteed.\n",
        "\n",
        "### Limitations\n",
        "- **Slow Mixing:** The chain may mix slowly when variables are strongly correlated.\n",
        "- **Dependency on Conditionals:** Requires that the conditional distributions are easy to sample from.\n",
        "- **Curse of Dimensionality:** In cases where the conditionals are high-dimensional or not analytically tractable, alternative methods may be preferred."
      ],
      "metadata": {
        "id": "j7gzjWGmkaN9"
      }
    }
  ]
}