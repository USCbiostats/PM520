{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNFPWyCMDmPdF86p9O3dOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab5_Optimization_PtII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP4Zigt0zfMW",
        "outputId": "12afeee3-14eb-471d-c012-76915c3dc59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lineax\n",
            "  Downloading lineax-0.0.4-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting equinox>=0.11.0 (from lineax)\n",
            "  Downloading equinox-0.11.3-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from lineax) (0.4.23)\n",
            "Collecting jaxtyping>=0.2.20 (from lineax)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from lineax) (4.9.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->lineax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->lineax) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->lineax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->lineax) (1.11.4)\n",
            "Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.20->lineax)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox, lineax\n",
            "Successfully installed equinox-0.11.3 jaxtyping-0.2.25 lineax-0.0.4 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install lineax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's Method for Optimization\n",
        "Recall under [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) we approximate our function $f(\\beta)$ as,\n",
        "$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)(\\beta - \\beta_t),$$ which suggests to locally minimize this approximation we find,\n",
        "$$\\nabla_\\beta f(\\beta) \\approx \\nabla f(\\beta_t).$$\n",
        "\n",
        "But, can we do better, by considering higher-order information (ie geometry) of\n",
        "the function $f$?\n",
        "\n",
        "Let's consider a 2nd-order [Taylor-series approximation](https://en.wikipedia.org/wiki/Taylor_series) to $f$ around $\\beta_t$ as,\n",
        "\n",
        "$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T H(\\beta_t)(\\beta - \\beta_t),$$ where $H(\\beta_t) = \\nabla^2 f(\\beta_t)$ (i.e. the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) of $f$ at $\\beta_t$). If we minimize this _local_ approximation, we see\n",
        "\n",
        "$\\nabla_\\beta f(\\beta) \\approx \\nabla f(\\beta_t) + \\nabla^2 f(\\beta_t)(\\beta - \\beta_t) = \\nabla f(\\beta_t) + H(\\beta_t)\\beta - H(\\beta_t)\\beta_t ⇒$\n",
        "$$ H(\\beta_t)\\beta = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t).$$\n",
        "\n",
        "We can recognize that this is a [system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $A x = b$ where $A = H(\\beta_t)$, $x = \\beta$, and $b = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)$. The solution is given by, $\\hat{x} = A^{-1}b$, which in this case implies,\n",
        "$$ \\hat{\\beta} = H(\\beta_t)^{-1}\\left(H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)\\right) = \\beta_t - H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n",
        "\n",
        "Contrast this with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), which is given by,\n",
        "$$ \\hat{\\beta} = \\beta_t - \\rho_t \\nabla f(\\beta_t).$$\n",
        "\n",
        "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is only guaranteed to converge _locally_, and can diverge even for _strongly_ [convex functions](https://en.wikipedia.org/wiki/Convex_function) (e.g., $f(\\beta) = \\sqrt{\\beta^2 + 1}$). To address this limitation, we can add a dampening parameter, $\\rho_t$, which gives us our final update form,\n",
        "$$ \\hat{\\beta} = H(\\beta_t)^{-1}(\\nabla^2 f(\\beta_t)\\beta_t - \\nabla f(\\beta_t)) = \\beta_t - \\rho_t H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n",
        "\n",
        "## Quasi-Newton Methods for Optimization\n",
        "What if computing $H(\\beta_t)$ is prohibitive or too costly? Do we need _exact_ second order information to improve on gradient descent's convergence? Given an approximation of $H$, called $B$, i.e. $B(\\beta_t) \\approx H(\\beta_t)$, [_quasi_-Newton methods](https://en.wikipedia.org/wiki/Quasi-Newton_method) optimize for the form\n",
        "$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T B(\\beta_t)(\\beta - \\beta_t),$$ where $B(\\beta_t) \\approx H(\\beta_t)$. Optimizing this statement gives us our update rule,\n",
        "$$ \\hat{\\beta} = \\beta_t - \\rho_t B(\\beta_t)^{-1}\\nabla f(\\beta_t).$$"
      ],
      "metadata": {
        "id": "_b5I2Q37z4_1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNUB6IIICara"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Poisson Regression\n",
        "\n",
        "$$y_i | x_i \\sim \\text{Poi}(\\lambda_i)$$ where $\\lambda_i := \\exp(x_i^T \\beta)$, and $\\text{Poi}(k | \\lambda) := \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$. Given $\\{(y_i, x_i)\\}_{i=1}^n$, we would like to identify the maximum likelihood parameter estimate for $\\beta$. In other words, we would to find a value for $\\beta$ such that we maximize the log-likelihood given by,\n",
        "$$\\begin{align*}\n",
        "\\log \\ell(\\beta) &= \\sum_i \\log \\text{Poi}(y_i | \\exp(x_i^T \\beta)) \\\\\n",
        "&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta) \\exp(-\\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n",
        "&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n",
        "&= \\sum_i \\log \\left[\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))\\right] - \\log(y_i!) \\\\\n",
        "&= \\sum_i \\left[y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta) - \\log(y_i!)\\right] \\\\\n",
        "&= y^T X\\beta - \\exp(X\\beta)^T 1_n - O(1) \\\\\n",
        "&= y^T X\\beta - \\lambda^T 1_n - O(1),\n",
        "\\end{align*}$$\n",
        "where $\\lambda = \\{\\lambda_1, \\dotsc, \\lambda_n\\}.$\n",
        "\n",
        "\n",
        "$$ \\begin{align*}\n",
        "\\nabla_\\beta \\ell &= \\nabla_\\beta \\left[ y^T X\\beta - \\lambda^T 1_n \\right] \\\\\n",
        "&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\lambda^T 1_n] \\\\\n",
        "&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\exp(X\\beta)^T 1_n] \\\\\n",
        "&= y^T X - \\exp(X\\beta)^T X  \\\\\n",
        "&= y^T X - \\lambda^T X  \\\\\n",
        "&= X^T(y - \\lambda) \\\\\n",
        "\\nabla^2_{\\beta \\beta} \\ell &= \\nabla_{\\beta} X^T(y - \\lambda) \\\\\n",
        "&= \\nabla_{\\beta} \\left[X^T y - X^T \\lambda \\right] \\\\\n",
        "&= - X^T \\nabla_{\\beta}  \\lambda \\\\\n",
        "&= -X^T \\nabla_{\\beta}  \\exp(X\\beta) \\\\\n",
        "&= -X^T \\Lambda X,\n",
        "\\end{align*}$$\n",
        "where $\\Lambda = \\text{diag}(\\lambda)$, i.e. $\\Lambda_{ii} = \\lambda_i$ and $\\Lambda_{ij} = 0$ for $i \\neq j$.\n",
        "\n",
        "We can fit using Newton's method. =>\n",
        "$$\\begin{align*}\n",
        "\\beta(t+1) &= \\beta(t) - H(\\beta(t))^{-1}\\nabla \\ell(\\beta_t) \\\\\n",
        "&= \\beta(t) + (X^T \\Lambda(t) X)^{-1} X^T (y - \\lambda) ⇒ \\\\\n",
        "&= (X^T \\Lambda(t) X)^{-1} X^T \\Lambda(t) (\\Lambda(t)^{-1}y + X\\beta(t) - 1)\n",
        "\\end{align*}$$\n",
        "where $\\Lambda(t) := \\text{diag}(\\lambda_1, \\dotsc, \\lambda_n)$."
      ],
      "metadata": {
        "id": "crQSiZATzm9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "import jax.scipy.stats as stats\n",
        "\n",
        "import lineax as lx\n",
        "\n",
        "@jax.jit\n",
        "def loglikelihood_eta(eta, y, X):\n",
        "  \"\"\"\n",
        "  Our loglikelihood function for $y_i | x_i ~ \\text{Poi}(\\exp(eta_i))$.\n",
        "\n",
        "  eta: X @ beta; the linear component for each observation\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix\n",
        "\n",
        "  returns: sum of the logliklihoods of each sample\n",
        "  \"\"\"\n",
        "  mean_lambda = jnp.exp(eta)\n",
        "  return jnp.sum(stats.poisson.logpmf(y, mean_lambda))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def irwls_fit(eta, y, X, step_size):\n",
        "  \"\"\"\n",
        "  Perform MLE estimation for $\\beta$ under the model\n",
        "     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
        "\n",
        "  eta: X @ beta; the linear component for each observation\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix\n",
        "\n",
        "  returns: updated estimate of $\\beta$\n",
        "  \"\"\"\n",
        "  # compute lambda_i := exp(x_i @ beta)\n",
        "  d_i = jnp.exp(eta)\n",
        "  d_sqrt = jnp.sqrt(d_i)\n",
        "\n",
        "  # compute z_i := Lambda^{1/2}(Lambda^-1 y + X @beta - 1)\n",
        "  z = (y / d_i + eta - 1) * d_sqrt\n",
        "\n",
        "  # X* := Lambda^{1/2} X\n",
        "  # we use linear operators to postpone any computation\n",
        "  X_star = lx.DiagonalLinearOperator(d_sqrt) @ X\n",
        "\n",
        "  # lineax can solve normal equations iteratively as (t(X*) @ (X* @ guess)) - z\n",
        "  solution = lx.linear_solve(X_star, z, solver=lx.NormalCG(atol=1e-4, rtol=1e-3))\n",
        "  beta = solution.value\n",
        "  return beta\n",
        "\n",
        "\n",
        "def poiss_reg(y, X, fit_func, step_size = 1.0, max_iter=100, tol=1e-3):\n",
        "  \"\"\"\n",
        "  Perform MLE estimation for $\\beta$ under the model\n",
        "     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
        "\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix\n",
        "  max_iter: the maximum number of iterations to perform optimization\n",
        "  tol:\n",
        "\n",
        "  returns: updated estimate of $\\beta$\n",
        "  \"\"\"\n",
        "  # intialize eta := X @ beta\n",
        "  eta = jnp.log((y + jnp.mean(y))/2)\n",
        "\n",
        "  # fake bookkeeping\n",
        "  loglike = -100000\n",
        "  delta = 10000\n",
        "\n",
        "  # convert to a linear operator for lineax\n",
        "  X = lx.MatrixLinearOperator(X)\n",
        "  for epoch in range(max_iter):\n",
        "\n",
        "    # fit using our function\n",
        "    beta = fit_func(eta, y, X, step_size)\n",
        "\n",
        "    # update eta\n",
        "    eta = X.mv(beta)\n",
        "\n",
        "    # evaluate log likelihood\n",
        "    newll = loglikelihood_eta(eta, y, X)\n",
        "\n",
        "    # take delta and check if we can stop\n",
        "    delta = jnp.fabs(newll - loglike)\n",
        "    print(f\"Epoch[{epoch}] = {newll}\")\n",
        "    if delta < tol:\n",
        "      break\n",
        "\n",
        "    # replace old value\n",
        "    loglike = newll\n",
        "\n",
        "  return beta"
      ],
      "metadata": {
        "id": "DKK1oqZMztkU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's simulate a poisson regression model with N samples and P variables\n",
        "N = 1000\n",
        "P = 5\n",
        "\n",
        "# initialize PRNG env\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "# split key for each random call\n",
        "key, y_key, x_key, b_key = rdm.split(key, 4)\n",
        "X = rdm.normal(x_key, shape=(N, P))\n",
        "beta = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "# compute lambda_i\n",
        "mean_lambda = jnp.exp(X @ beta)\n",
        "\n",
        "# sample y from Poi(lambda_i)\n",
        "y = rdm.poisson(y_key, mean_lambda)\n",
        "\n",
        "# estimate beta using our irwls function\n",
        "# fit_func has signature (eta, y, X, step_size)\n",
        "beta_hat = poiss_reg(y, X, irwls_fit)\n",
        "print(f\"beta = {beta}\")\n",
        "print(f\"hat(beta) = {beta_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRai4qWiz1_R",
        "outputId": "092a02fe-7d1a-4a88-f99a-b8f3a6134ab7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[0] = -1393.6072998046875\n",
            "Epoch[1] = -1374.225341796875\n",
            "Epoch[2] = -1374.12451171875\n",
            "Epoch[3] = -1374.12451171875\n",
            "beta = [ 0.85658115  0.36212853  1.1522139   0.15692838 -0.35338545]\n",
            "hat(beta) = [ 0.8356752   0.3820638   1.1501597   0.15936355 -0.35638028]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's implement poisson regression using _only_ gradient informatino to perform inference\n",
        "# and measure how quickly it converges compared with the Newton method\n",
        "def grad_fit(eta, y, X, step_size):\n",
        "  beta = ...\n",
        "  return beta"
      ],
      "metadata": {
        "id": "ZalxS2NOOfiV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation"
      ],
      "metadata": {
        "id": "WbJ1nlWE0R7T"
      }
    }
  ]
}