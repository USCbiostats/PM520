{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF3EOg+N4ECmpgMXGSWlzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/HW/PM520_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3. Variational Inference"
      ],
      "metadata": {
        "id": "iVleC9nFW7sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Evidence Lower Bound\n",
        "$\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\by}{\\mathbf{y}}\\newcommand{\\bI}{\\mathbf{I}}$\n",
        "Recall from Lab 8, our example of variational inference for a Bayesian linear regression model. Namely,\n",
        "$$\\begin{align*}\n",
        "\\by | \\bX, \\beta &\\sim N(\\bX\\beta, \\bI_n \\sigma^2) \\\\\n",
        "\\beta &\\sim N(0, \\bI_p \\sigma^2_b).\n",
        "\\end{align*}$$\n",
        "\n",
        "We assumed a mean-field model that $Q$ factorizes as $$Q(\\beta) = \\prod_{j=1}^P Q_j(\\beta_j).$$\n",
        "\n",
        "### 1.1\n",
        "Consulting the results in Lab 8 on parameter definitions for each $Q_j$, please derive the *evidence lower bound* or ELBO for this model.\n",
        "\n",
        "### 1.2\n",
        "Consult lab 8 for the implementation of a CAVI algorithm for the model above, but rather than evaluate the mean squared error (MSE), evaluate the ELBO. The ELBO should *increase* with each iteration, otherwise there is likely a bug."
      ],
      "metadata": {
        "id": "kVq8dl-TXbD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bayesian Linear Regression Pt II\n",
        "Here we assume a slightly different linear model, which is given by, $$\\begin{align*}\n",
        "\\by | \\bX, \\beta &\\sim N(\\bX\\beta, \\bI_n \\sigma^2) \\\\\n",
        "\\beta_j &\\sim \\text{Laplace}(0, b).\n",
        "\\end{align*}$$\n",
        "\n",
        "We assumed a mean-field model that $Q$ factorizes as $$Q(\\beta) = \\prod_{j=1}^P Q_j(\\beta_j).$$ Rather than identify optimal $Q_j$ through CAVI, we will first assume $Q_j := \\text{Laplace}(\\mu_j, b_j)$. Next, to identify updates for each $\\mu_j, b_j$, we take the derivative of the ELBO with respect to each; however the gradient of the ELBO requires knowing $\\mu_j, b_j$, which causes challenges.\n",
        "\n",
        "### 2.1\n",
        "Re-write the ELBO as a deterministic transformation of $\\beta_j$ using location-scale rules (i.e. reparameterization trick)\n",
        "\n",
        "### 2.2\n",
        "Implement the above by performing stochastic VI to optimize the ELBO by sampling.\n",
        "\n"
      ],
      "metadata": {
        "id": "WehxWW76ZM_A"
      }
    }
  ]
}